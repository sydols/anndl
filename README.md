# Real or Not: Detecting AI-Generated Images with Deep Learning
In the age of generative AI, fake images are becoming nearly indistinguishable from real ones. From artistic creations to fake news, these hyper-realistic visuals pose both creative opportunities and ethical concerns. The ability to detect AI-generated images has become a critical task for ensuring media authenticity, combating misinformation, and maintaining trust in digital content. I aim to answer a critical question: Can we train a neural network to reliably distinguish between AI-generated and real photographs?
As AI-generated content floods the internet, we need better tools to detect and understand what’s real and what’s not. I will explore this issue by training and evaluating neural network models on the CIFAKE dataset from Kaggle. 

![dataset-card](https://github.com/user-attachments/assets/77af91c0-4b08-4758-9914-8af44ae91ad6)

All images are RGB and 256×256 in size. It includes 60,000 real images collected from Krizhevsky & Hinton's CIFAR-10 dataset; 60,000 fake images generated to be equivalent to CIFAR-10 with Stable Diffusion version 1.4. The data is split into 100,000 training images (50,000 real, 50,000 fake) and 20,000 testing images (10,000 real, 10,000 fake). Each image is pre labeled as either: Real (human-captured images) or Fake (generated by AI systems). The dataset is balanced and perfect for binary classification. During preprocessing, each image was resized to 64x64 pixels whose values were normalized to [0, 1].
The first model I introduced to the dataset was logistic regression on the flattened image pixels. Each individual pixel was broken down into a feature. While it’s a good model to directly compare images. It has no spatial awareness and cannot detect patterns and nearby pixels. 

<img width="444" alt="logregcode" src="https://github.com/user-attachments/assets/dd1a10b9-cc46-49e5-8a38-f42a5b5a8c8d" />

I expected this model to perform poorly. But after ten epochs, the accuracy was 0.8333 and the loss was 0.5344. 

![logregacc](https://github.com/user-attachments/assets/300f4d67-5be0-4c5c-bc1a-ccec6009ad50)
![logregloss](https://github.com/user-attachments/assets/21706111-3116-41a5-a2d7-9c122f987b44)
<img width="370" alt="logregclass" src="https://github.com/user-attachments/assets/f8e66e45-8131-4a3c-b2f2-46aba5896423" />

I also introduced a classical CNN to the CIFAKE dataset to see what my baseline would be before I started tweaking a custom CNN. This CNN applies thirty-two filters with a 3x3 kernel to extract low-level features; Downsamples feature maps to reduce spatial size and computation; Applies sixty-four filters with a 3x3 kernel to extract more complex patterns; And converts 2D feature maps into 1D vectors.

<img width="628" alt="classcnncode" src="https://github.com/user-attachments/assets/d8a2b844-5b0b-42a1-84b1-b6204875cde4" />

I expected this model to perform well, and it did.

![classcnnacc](https://github.com/user-attachments/assets/3e857019-be06-4478-8391-1d7683b84718)
![classcnnloss](https://github.com/user-attachments/assets/1ffdfbf2-b18b-40c9-8728-744992f8697c)
<img width="369" alt="classcnnclass" src="https://github.com/user-attachments/assets/6ef210f4-287b-44b7-9b17-a884d9834fa3" />

Both of the baseline models’ outcomes are measured by plotting training/validation accuracy and loss, confusion matrices, and calculating precision, recall, and F1-Scores. Both models performed surprisingly well.
My two-block custom CNN expects color images of 64 by 64 pixels with three channels for rgb. Block one focuses on feature extraction and regularization. It extracts sixteen low-level features and keeps output size the same as the input size. It adds L2 regularization to prevent overfitting and normalizes activations for faster and more stable training. It downsamples feature maps and randomly drops 30% of the neurons to improve generalization. 

<img width="487" alt="customblock1" src="https://github.com/user-attachments/assets/1b7ec27c-7a71-43ec-b555-d1238b799f4b" />

Block two is similar to block one, but focuses on deeper features and stronger regularization. It has thirty-two filters and a stronger random neuron dropout of 40%. 

<img width="472" alt="customblock2" src="https://github.com/user-attachments/assets/84b57568-5179-4cea-b680-b5a47284a5db" />

Lastly, the custom CNN reduces each feature map to a single value to reduce overfitting and parameter count, combines learned features, randomly drops 50% of neurons, and outputs probability for binary classification.

<img width="319" alt="custompool" src="https://github.com/user-attachments/assets/85c18966-61e4-485b-8a0d-dc13055a956c" />

The custom CNN performed slightly better if not the same as logistic regression and classic CNN models.

![CustomCNNAccuracy](https://github.com/user-attachments/assets/744ae98f-0337-4384-bf1b-e7095a2e209b)
![CustomCNNLoss](https://github.com/user-attachments/assets/4b76ec50-8be5-49c8-99fb-366e279b0a11)

To have something more polished to compare the custom CNN to, I used the MobileNetV2 architecture which has been pre trained on ImageNet. I used the base model and added some simple classifiers to it to better fit my dataset. 

<img width="597" alt="mobilecode" src="https://github.com/user-attachments/assets/fe66b2b8-5a04-401d-ba9f-279a87bbbca7" />

Surprisingly, this had similar results as my custom CNN which makes me think that I’ve hit a glass ceiling on this data set or I don’t have the tools necessary to fine tune a model into 90% accuracy. 

![MobileNetV2Loss](https://github.com/user-attachments/assets/4e9017fb-aa7c-4e28-982d-e7ac6fa83129)

Logistic regression and the classical CNN both performed better than I expected so working on improving their accuracy was something I struggled with. I tweaked my custom CNN for a couple days before settling on a two block method to reduce overfitting and improve generalization. It was difficult to juggle run-time while giving the model a fair chance at succeeding the classical CNN and logistic regression models. I settled on giving each model the same input image size and the same amount of epochs to keep it fair and dealt with the longer run times. 

At the end of the day, the differences between the custom CNN, MobileNetV2, logistic regression, and classical CNN models are not statistically significant enough to label as differences. Whether this has to do with the data itself, my skills, or the fact that maybe it is hard for neural networks to consistently identify AI-generated versus real images. However, even though not statistically significantly different, each model did pretty well labeling each image with an average accuracy of 83%. I would say that it is possible for neural networks to identify AI generated images which brings hope to the future of media literacy.
